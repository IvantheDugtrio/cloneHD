{"name":"cloneHD","tagline":"High-definition reconstruction of clonal composition from next-generation sequencing data","body":"# How to get cloneHD and filterHD?\r\n\r\nThe current stable release, including pre-compiled executable binaries\r\nof filterHD and cloneHD for Mac OS X (64bit), can be found at\r\n<ftp://ftp.sanger.ac.uk/pub/teams/153/cloneHD/>.  The source code can\r\nalso be downloaded here.\r\n\r\n# Run a test with simulated data\r\n\r\nIf you download cloneHD from the ftp site above, you can test both filterHD and cloneHD by running\r\n\r\n`$ sh run-example.sh`\r\n\r\nwhere you can see a typical workflow of analysing read depth and BAF\r\ndata with a matched normal. All command line arguments are explained below.\r\n\r\n# Compilation  \r\n\r\nTo compile cloneHD yourself, you need the GNU scientific library ([GSL](http://www.gnu.org/software/gsl/)) v1.15 or later. Change the paths in the Makefile to your GSL installation location (if non-standard). Then type \r\n\r\n`$ make`\r\n\r\nin the source directory. The two executables, `filterHD` and\r\n`cloneHD`, will be in `./build`.\r\n\r\n# What are cloneHD and filterHD for?\r\n\r\ncloneHD is a software for reconstructing the subclonal structure of a\r\npopulation from next-generation short-read sequencing data. Read depth\r\ndata, B-allele count data and somatic nucleotide variant (SNV) data can be\r\nused for the inference. cloneHD can find the number of subclonal\r\npopulations, their copy number profiles, their B-allele status and all\r\nSNV genotypes with high resolution.\r\n\r\nfilterHD is a general purpose probabilistic filtering algorithm for one-dimensional\r\ndiscrete data, similar in spirit to a Kalman filter. It is a continuous state\r\nspace Hidden Markov model with Poisson or Binomial emissions and a\r\njump-diffusion propagator. It can be used for scale-free smoothing, \r\nfuzzy data segmentation and data filtering. \r\n\r\n![cna gof](/images/cna.gof.png \"CNA goodness of fit\")\r\n![cna gof](/images/cna.post.png \"CNA posterior\")\r\n![baf gof](/images/baf.gof.png \"BAF goodness of fit\")\r\n![cna gof](/images/baf.post.png \"BAF posterior\")\r\n\r\nVisualization of the cloneHD output for the simulated data set. From\r\ntop to bottom: (i) the bias corrected read depth data and the cloneHD\r\nposterior mean emission rate (ii) the total copy number posterior\r\ndistribution for subclone 1 with f1=0.52 and subclone 2 with f2=0.07\r\n(iii) the BAF and (iv) the minor allele posterior. (Plots created with Wolfram\r\n[Mathematica](http://www.wolfram.com/mathematica/).)\r\n\r\n# filterHD command line arguments\r\n\r\n## Typical usage options\r\n\r\n*    `--data [file]`  Input data. \r\n\r\n     The file format is the same as below for `--cna`, `--baf` or\r\n     `--snv`. Samples are processed one by one.\r\n\r\n*    `--mode [1/2/3/4]`  Emission modes.\r\n\r\n        1. Binomial (for SNV data and BAF data (use with `--reflect 1`))\r\n        2. Beta-Binomial (over-dispersed Binomial)\r\n        3: Poisson (for read depth data) \r\n        4: Negative-Binomial (over-dispersed Poisson)\r\n\r\n    In modes 3/4, the range of the hidden emission rate is learned\r\n    automatically. For modes 1/2, it is always in [0,1]. Reflective\r\n    boundary conditions are used.\r\n\r\n*    `--pre [string:\"./out\"]`  Prefix for all output files.\r\n\r\n*    `--dist [0/1:0]`  Whether to print the  posterior distribution. \r\n\r\n     Files can be big. The posterior mean, std-dev and\r\n     jump probability are printed in all cases to files\r\n     `*posterior.*.txt`, one for each sample in the input.\r\n\r\n*    `--jumps [0/1:0]`  Whether to print posterior jump probability. \r\n\r\n     The posterior jump probability is compounded over all samples. It\r\n     can be used with `--min-jump [double]` below, to consolidate jumps..\r\n\r\n*    `--reflect [0/1:0]`  If 1, binomial observations `n in N` and\r\n     `(N-n) in N` are assumed to be identical. Use this option for BAF data.\r\n\r\n## Parameter options\r\n\r\nThe HMM underlying filterHD is determined by these four\r\nparameters. They can all be fixed, otherwise they are learned from the data.\r\n\r\n*    `--jump [double]`  The jump probability per length unit (bp).\r\n*    `--sigma [double]`  The diffusion constant. \r\n*    `--shape [double]`  The shape parameter for modes 2/4. If >1000, use modes 1/3.\r\n*    `--rnd [double]`  The rate of random emissions.\r\n\r\nFor all of the above parameters, initial values for the numerical\r\noptimization can be given. This might be useful if you suspect several\r\nlocal optima.\r\n\r\n*    `--jumpi [double]`\r\n*    `--sigmai [double]`\r\n*    `--shapei [double]`\r\n*    `--rndi [double]`\r\n\r\n## Advanced options\r\n\r\n*    `--min-jump [double:0.0]`  Consolidate jumps down to `--min-jump`.\r\n\r\n     The posterior jump probability track will be consolidated by merging neighboring jump events into\r\n     unique jumps, down to the minimum value given here. Can only be used with\r\n     `--jumps 1`. \r\n\r\n*    `--filter-pVal [0/1:0]`  Use p-Val filter.\r\n\r\n     Filter sites where the p-Value of the\r\n     observation is below 10/nSites, where nSites is the total number\r\n     of sites in a sample.\r\n\r\n*    `--filter-shortSeg [int:0]` Use short segment filter.\r\n\r\n     Filter sites within short segments between jumps. All filtered data will be in the file ending `*filtered.txt`.\r\n\r\n*    `--grid [int:100]`  Set grid size.\r\n\r\n     The grid size for the internal representation of continuous distributions. For large ranges in\r\n     mode 3/4, it can make sense to increase resolution.\r\n\r\n\r\n\r\n# cloneHD command line arguments\r\n\r\n## Typical usage options\r\n\r\nFormat of input files: the first two columns of all three input file\r\n    types are always chromosome and coordinate of each observation. Chromosomes\r\n    are expected to be integers (X->23,Y->24). \r\n\r\n*   `--cna [file]` Read depth data file. \r\n\r\n    Format: For each sample, there are two additional columns with\r\n    (i) the read depth and (ii) the number of independent observations\r\n    this is the sum of. For human NGS data, use the mean read depth\r\n    per 1 kb as the highest resolution. \r\n\r\n        1  1000  93   1  75   1  etc.\r\n        1  2000  101  1  81   1\r\n        1  3000  105  1  85   1\r\n        1  5000  197  2  156  2\r\n        etc.\r\n\r\n*    `--baf [file]` B-allele read count data file. \r\n\r\n     Format: For each sample, there are two additional columns with\r\n     (i) the number of reads of the minor allele and (ii) the total\r\n     read depth at originally heterozygous loci.\r\n\r\n        1  1036  43  90   28  72  etc.\r\n        1  1287  47  99   32  80\r\n        1  2877  30  100  36  82\r\n        etc.\r\n\r\n*    `--snv [file]` Somatic nucleotide variant read count data file.\r\n\r\n     Format: For each sample, there are two additional columns with (i) the number of reads of the somatic variant allele and (ii) the total read depth at that locus.\r\n\r\n        1  1314  12  92   28  72  etc.\r\n        1  1287  47  99   32  80\r\n        1  2877  30  100  36  82\r\n        etc.\r\n\r\n*    `--pre [string:\"./out\"]`  Prefix for all output files.\r\n\r\n*    `--bias [file]`  The bias field for the read depth data. \r\n\r\n     This must be a filterHD `*posterior.*.txt` file, typically from a\r\n     filterHD run on matched-normal read depth data, to estimate the\r\n     technical read depth modulation.\r\n\r\n*    `--max-tcn [int]`  The maximum total copy number.\r\n\r\n     This is used as an upper limit for the total copy number genome wide (in all chr).\r\n     If not specified, the normal copy number is used as limit for each chromosome.\r\n\r\n     This number should be chosen conservatively, since it increases the\r\n     HMM dimensionality and can open up the possibility for spurious solutions. \r\n\r\n*    `--nmax [int:2]`  The maximum number of subclones to be tried.\r\n\r\n     All subclone numbers from 0 to `nmax` will be used and the one\r\n     with maximum BIC chosen for output.\r\n\r\n*    `--force [int]`  Fix the number of subclones to be used.\r\n\r\n*    `--trials [int:1]`  The number of independent optimizations.\r\n\r\n     Global parameters are found numerically by local maximization of\r\n     the total log-likelihood. The best result out of `trials` independent,\r\n     randomly seeded,  runs will be used.\r\n\r\n*    `--mean-tcn [file]`  Use a fixed mean total copy number for SNV data. \r\n\r\n     For a SNV data analysis, the cloneHD output file\r\n     ending `*mean_tcn.txt` from a CNA(+BAF) run can be supplied here. Since\r\n     the subclonal decomposition can be different for SNVs, this option\r\n     ensures that a reasonable mean total copy number is used.\r\n\r\n*    `--avail-cn [file]`  Use SNV copy number availablility constraint.\r\n\r\n     For a SNV data analysis, the cloneHD output file\r\n     ending `*avail_cn.txt` from a CNA(+BAF) run can be supplied here. Since\r\n     the subclonal decomposition can be different for SNVs, this option\r\n     ensures that the SNV genotype is consistent with the fraction of\r\n     cells in which this number of copies is available.\r\n     Can only be used together with `--mean-tcn [file]`. In\r\n     combination, this is a much stronger constraint than using\r\n     `mean-tcn` alone.\r\n\r\n### Fuzzy segmentation options\r\n\r\nFor data with persistence along the genome, a fuzzy segmentation can\r\nbe used based on the filterHD posterior jump probability (must be\r\n`*jumps.txt` file). Data between potential jump sites, with a jump\r\nprobability of at least `min-jump`, is collapsed. The jump probability\r\nis used in the HMM transition.\r\n\r\n*    `--cna-jumps [file]`\r\n*    `--baf-jumps [file]`\r\n*    `--snv-jumps [file]`\r\n*    `--min-jump [double:0.01]` \r\n\r\n## Parameter options\r\n\r\nThe shape parameter for the over-dispersed emission models\r\n(Negative-Binomial or Beta-Binomial). If not specified, the normal\r\nmodels are used (Poisson or Binomial).\r\n\r\n*    `--cna-shape [double:inf]`\r\n*    `--baf-shape [double:inf]`\r\n*    `--snv-shape [double:inf]`\r\n\r\nThe rate for indiviual random emissions per data set. Can be learned\r\nwith filterHD for data with persistence.\r\n\r\n*    `--cna-rnd [double:0.0]`\r\n*    `--baf-rnd [double:0.0]`\r\n*    `--snv-rnd [double:0.0]`\r\n\r\n## Advanced options\r\n\r\n*    `--clones [file]`  Use fixed mass(es) and/or subclonal frequencies. \r\n\r\n     Either all mass parameters, or all subclonal frequencies, or both \r\n     can be given (for each sample in the data input). The likelihoods\r\n     and posteriors will be computed under these conditions. \r\n     Remaining parameters will be learned.\r\n  \r\n     Format: One line per sample. The first column, if greater than\r\n     1.0, is interpreted as mass; the remaining as subclonal frequencies.\r\n\r\n        30.0 0.64 0.12\r\n        28.0 0.31 0.23\r\n\r\n    More than one parameter set can be given (as a continued list). Then,\r\n    only the likelihoods are computed and printed to a file ending\r\n    `*llh-values.txt`.  Useful for mapping the log-likelihood surface\r\n    or comparing several given solutions.\r\n\r\n*    `--purity [file]`  Use fixed purities, i.e. lower bounds for the sum of\r\n     subclonal frequencies. One line per sample.\r\n\r\n*    `--restarts [int:10]`  The number of perturbations in local random\r\n     search mode.\r\n\r\n     This simple random search routine is used: after finding a local\r\n     maximum of LLH, the best solution is perturbed and a new optimum\r\n     is sought. \r\n\r\n*    `--seed [int]`  A fixed seed to make inferences reproducible.\r\n\r\n*    `--mass-gauging [0/1:1]`  Whether to use mass-gauging.\r\n\r\n     The optimization in the space of masses (seq depths per haploid\r\n     DNA) and subclonal frequencies can suffer from many local\r\n     optima.  To fix the mass(es), one can, for a given solution,\r\n     assume that an occupied state is actually all-normal. All\r\n     occupied states will be proposed to fix the mass(es) \r\n\r\n*    `--min-occ [double:0.01]`  The minimum occupancy of levels to be\r\n     used for the mass gauging.\r\n\r\n*    `--print-all [0/1:0]`  If 1, the posterior for every observation\r\n     is printed to files ending `*[cna/baf/snv].posterior.txt`. \r\n     If 0, only one line for each segment is printed.\r\n\r\n*    `--max-tcn [file]`  The maximum total copy number per chr and subclone.\r\n\r\n    This file should have the format: chr max1 max2 max3 etc., e.g.\r\n\r\n        1  2\r\n        2  2\r\n        3  8  2\r\n        4  2\r\n        etc.\r\n\r\n     The first column is the chromosome, the next columns are the limits to be used for subclone 1, 2 etc.\r\n     For subclones not specified, the limit in the last column is used. In the example above, subclone 1 has an upper limit of 8 total copies in chr3, for all other subclones and in all other chromosomes, the upper limit is 2. If only SNV data is provided (and `--avail-cn [file]` is not given), this is used to fix the total number of copies. If `--max-tcn` is not given, cloneHD uses the normal copy number for each chr.\r\n\r\n*    `--learn-priors [0/1:0]` For snv-mode only: if 1, then the parameters\r\n     for the multiplicative genotype priors are learned.\r\n\r\n*    `--chr [file]`  Set normal copy numbers.\r\n\r\n     The normal copy number for every single\r\n     chromosome can be specified. This is needed only for non-human DNA. If not\r\n     given, human DNA is assumed and the sex is inferred from the\r\n     presence or absence of chr 24 (= chr Y) in the input data.\r\n\r\n*    `--snv-fprate [double:1.0e-4]`  The false positive rate for SNVs,\r\n     i.e. rate of SNV data points of genotype all-0.\r\n\r\n*    `--snv-fpfreq [double:0.01]`  The typical frequency of false positive SNVs.\r\n\r\n*    `--snv-pen [double:0.01]`  The penalty for higher than expected\r\n     genotypes.\r\n\r\n*    `--baf-pen [double:1.0]`  The penalty for complex minor allele\r\n     status.\r\n\r\n*    `--cna-jump [double:-1.0]`\r\n*    `--baf-jump [double:-1.0]`\r\n*    `--snv-jump [double:-1.0]`\r\n\r\nA constant jump probability per base pair. If -1, then observations are\r\nuncorrellated along the genome. Can be learned with filterHD. No fuzzy\r\ndata segmentation is performed.  Useful in combination with\r\n`--clones`, where very high-definition information\r\navailable. Using this option will change the posterior output file format.\r\n\r\n## Bulk options\r\n\r\nThese options are only needed if the sequenced cell population is a mixture of a\r\ndiverse bulk, with known allele frequency profile, and a number of\r\nsubclones with unknown genotypes and frequencies. Allele frequency\r\ndata is input with `--snv`. Data segmentation can be used with\r\n`--snv-jumps`.  Read depth data can also be specified with `--cna`. \r\n\r\n*    `--bulk-mean [double]`  The bulk allele frequency profile. \r\n\r\n     Must be a filterHD `*posterior.*.txt` file. Only the posterior mean is used.\r\n\r\n*    `--bulk-prior [file]`  The bulk allele frequency profile. \r\n\r\n     Must be a filterHD `*posterior.*.txt` file. The whole posterior\r\n     distribution is used (run filterHD with `--dist 1` to obtain it).\r\n\r\n*    `--bulk-updates [int:0]`  The number of Bayesian updates of the\r\n     bulk allele frequency profile (if `--bulk-prior` was used).\r\n\r\n*    `--bulk-fix [double:0.0]`  Use a flat and fixed bulk allele\r\n     frequency profile.\r\n\r\n## Technical options\r\n\r\nThe grid sizes for the pre-computed emission probabilities if fuzzy data segmentation is used.\r\n\r\n*    `--cna-grid [int:300]`  \r\n*    `--baf-grid [int:100]` \r\n*    `--snv-grid [int:100]`  \r\n\r\n# Tips and tricks\r\n\r\n*  Pre-filtering of data can be very important. If filterHD predicts\r\n   many more jumps than you would expect, it might be necessary to\r\n   filter the data, removing very short segments (with\r\n   `--filter-shortSeg 10`).\r\n\r\n*  Make sure that the bias field for the tumor CNA data is\r\n   meaningful. If a matched normal sample was sequenced with the same\r\n   pipeline, its read depth profile, as predicted by filterHD, can be used as a\r\n   bias field for the tumor CNA data. Follow the logic of the example\r\n   given here.\r\n\r\n*  filterHD can sometimes run into local optima. It might be useful to\r\n   fix initial values for the parameters via `--jumpi [double]` etc.\r\n\r\n*  By default, cloneHD runs with mass gauging enabled. This seems like\r\n   an overkill, but is actually quite useful because you can see some\r\n   alternative explanations during the course of the analysis.\r\n\r\n*  Don't put too much weight on the BIC criterion. It was calibrated\r\n   using simulated data. For real data, it should be supplied with\r\n   common sense and biological knowledge. Use `--force [int]` to use a\r\n   fixed number of subclones.\r\n\r\n*  For exome sequencing data, the read depth bias can be enormous. Use rather, if\r\n   available, the jumps seen in the BAF data for both CNA and BAF.\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}