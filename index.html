<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="cloneHD : High-definition reconstruction of clonal composition from next-generation sequencing data" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>cloneHD</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/andrej-fischer/cloneHD">View on GitHub</a>

          <h1 id="project_title">cloneHD</h1>
          <h2 id="project_tagline">High-definition reconstruction of clonal composition from next-generation sequencing data</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/andrej-fischer/cloneHD/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/andrej-fischer/cloneHD/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a name="how-to-get-clonehd-and-filterhd" class="anchor" href="#how-to-get-clonehd-and-filterhd"><span class="octicon octicon-link"></span></a>How to get cloneHD and filterHD?</h1>

<p>The current stable release, including pre-compiled executable binaries
of filterHD and cloneHD for Mac OS X (64bit), can be found at
<a>ftp://ftp.sanger.ac.uk/pub/teams/153/cloneHD/</a>.  The source code can
also be downloaded here.</p>

<h1>
<a name="run-a-test-with-simulated-data" class="anchor" href="#run-a-test-with-simulated-data"><span class="octicon octicon-link"></span></a>Run a test with simulated data</h1>

<p>If you download cloneHD from the ftp site above, you can test both filterHD and cloneHD by running</p>

<p><code>$ sh run-example.sh</code></p>

<p>where you can see a typical workflow of analysing read depth and BAF
data with a matched normal. All command line arguments are explained below.</p>

<h1>
<a name="compilation" class="anchor" href="#compilation"><span class="octicon octicon-link"></span></a>Compilation</h1>

<p>To compile cloneHD yourself, you need the GNU scientific library (<a href="http://www.gnu.org/software/gsl/">GSL</a>) v1.15 or later. Change the paths in the Makefile to your GSL installation location (if non-standard). Then type </p>

<p><code>$ make</code></p>

<p>in the source directory. The two executables, <code>filterHD</code> and
<code>cloneHD</code>, will be in <code>./build</code>.</p>

<h1>
<a name="what-are-clonehd-and-filterhd-for" class="anchor" href="#what-are-clonehd-and-filterhd-for"><span class="octicon octicon-link"></span></a>What are cloneHD and filterHD for?</h1>

<p>cloneHD is a software for reconstructing the subclonal structure of a
population from next-generation short-read sequencing data. Read depth
data, B-allele count data and somatic nucleotide variant (SNV) data can be
used for the inference. cloneHD can find the number of subclonal
populations, their copy number profiles, their B-allele status and all
SNV genotypes with high resolution.</p>

<p>filterHD is a general purpose probabilistic filtering algorithm for one-dimensional
discrete data, similar in spirit to a Kalman filter. It is a continuous state
space Hidden Markov model with Poisson or Binomial emissions and a
jump-diffusion propagator. It can be used for scale-free smoothing, 
fuzzy data segmentation and data filtering. </p>

<p><img src="/images/cna.gof.png" alt="cna gof" title="CNA goodness of fit"><img src="/images/cna.post.png" alt="cna gof" title="CNA posterior"><img src="/images/baf.gof.png" alt="baf gof" title="BAF goodness of fit"><img src="/images/baf.post.png" alt="cna gof" title="BAF posterior"></p>

<p>Visualization of the cloneHD output for the simulated data set. From
top to bottom: (i) the bias corrected read depth data and the cloneHD
posterior mean emission rate (ii) the total copy number posterior
distribution for subclone 1 with f1=0.52 and subclone 2 with f2=0.07
(iii) the BAF and (iv) the minor allele posterior. (Plots created with Wolfram
<a href="http://www.wolfram.com/mathematica/">Mathematica</a>.)</p>

<h1>
<a name="filterhd-command-line-arguments" class="anchor" href="#filterhd-command-line-arguments"><span class="octicon octicon-link"></span></a>filterHD command line arguments</h1>

<h2>
<a name="typical-usage-options" class="anchor" href="#typical-usage-options"><span class="octicon octicon-link"></span></a>Typical usage options</h2>

<ul>
<li>
<p><code>--data [file]</code>  Input data. </p>

<p>The file format is the same as below for <code>--cna</code>, <code>--baf</code> or
 <code>--snv</code>. Samples are processed one by one.</p>
</li>
<li>
<p><code>--mode [1/2/3/4]</code>  Emission modes.</p>

<pre><code>1. Binomial (for SNV data and BAF data (use with `--reflect 1`))
2. Beta-Binomial (over-dispersed Binomial)
3: Poisson (for read depth data) 
4: Negative-Binomial (over-dispersed Poisson)
</code></pre>

<p>In modes 3/4, the range of the hidden emission rate is learned
automatically. For modes 1/2, it is always in [0,1]. Reflective
boundary conditions are used.</p>
</li>
<li><p><code>--pre [string:"./out"]</code>  Prefix for all output files.</p></li>
<li>
<p><code>--dist [0/1:0]</code>  Whether to print the  posterior distribution. </p>

<p>Files can be big. The posterior mean, std-dev and
 jump probability are printed in all cases to files
 <code>*posterior.*.txt</code>, one for each sample in the input.</p>
</li>
<li>
<p><code>--jumps [0/1:0]</code>  Whether to print posterior jump probability. </p>

<p>The posterior jump probability is compounded over all samples. It
 can be used with <code>--min-jump [double]</code> below, to consolidate jumps..</p>
</li>
<li><p><code>--reflect [0/1:0]</code>  If 1, binomial observations <code>n in N</code> and
 <code>(N-n) in N</code> are assumed to be identical. Use this option for BAF data.</p></li>
</ul><h2>
<a name="parameter-options" class="anchor" href="#parameter-options"><span class="octicon octicon-link"></span></a>Parameter options</h2>

<p>The HMM underlying filterHD is determined by these four
parameters. They can all be fixed, otherwise they are learned from the data.</p>

<ul>
<li>   <code>--jump [double]</code>  The jump probability per length unit (bp).</li>
<li>   <code>--sigma [double]</code>  The diffusion constant. </li>
<li>   <code>--shape [double]</code>  The shape parameter for modes 2/4. If &gt;1000, use modes 1/3.</li>
<li>   <code>--rnd [double]</code>  The rate of random emissions.</li>
</ul><p>For all of the above parameters, initial values for the numerical
optimization can be given. This might be useful if you suspect several
local optima.</p>

<ul>
<li>   <code>--jumpi [double]</code>
</li>
<li>   <code>--sigmai [double]</code>
</li>
<li>   <code>--shapei [double]</code>
</li>
<li>   <code>--rndi [double]</code>
</li>
</ul><h2>
<a name="advanced-options" class="anchor" href="#advanced-options"><span class="octicon octicon-link"></span></a>Advanced options</h2>

<ul>
<li>
<p><code>--min-jump [double:0.0]</code>  Consolidate jumps down to <code>--min-jump</code>.</p>

<p>The posterior jump probability track will be consolidated by merging neighboring jump events into
 unique jumps, down to the minimum value given here. Can only be used with
 <code>--jumps 1</code>. </p>
</li>
<li>
<p><code>--filter-pVal [0/1:0]</code>  Use p-Val filter.</p>

<p>Filter sites where the p-Value of the
 observation is below 10/nSites, where nSites is the total number
 of sites in a sample.</p>
</li>
<li>
<p><code>--filter-shortSeg [int:0]</code> Use short segment filter.</p>

<p>Filter sites within short segments between jumps. All filtered data will be in the file ending <code>*filtered.txt</code>.</p>
</li>
<li>
<p><code>--grid [int:100]</code>  Set grid size.</p>

<p>The grid size for the internal representation of continuous distributions. For large ranges in
 mode 3/4, it can make sense to increase resolution.</p>
</li>
</ul><h1>
<a name="clonehd-command-line-arguments" class="anchor" href="#clonehd-command-line-arguments"><span class="octicon octicon-link"></span></a>cloneHD command line arguments</h1>

<h2>
<a name="typical-usage-options-1" class="anchor" href="#typical-usage-options-1"><span class="octicon octicon-link"></span></a>Typical usage options</h2>

<p>Format of input files: the first two columns of all three input file
    types are always chromosome and coordinate of each observation. Chromosomes
    are expected to be integers (X-&gt;23,Y-&gt;24). </p>

<ul>
<li>
<p><code>--cna [file]</code> Read depth data file. </p>

<p>Format: For each sample, there are two additional columns with
(i) the read depth and (ii) the number of independent observations
this is the sum of. For human NGS data, use the mean read depth
per 1 kb as the highest resolution. </p>

<pre><code>1  1000  93   1  75   1  etc.
1  2000  101  1  81   1
1  3000  105  1  85   1
1  5000  197  2  156  2
etc.
</code></pre>
</li>
<li>
<p><code>--baf [file]</code> B-allele read count data file. </p>

<p>Format: For each sample, there are two additional columns with
 (i) the number of reads of the minor allele and (ii) the total
 read depth at originally heterozygous loci.</p>

<pre><code>1  1036  43  90   28  72  etc.
1  1287  47  99   32  80
1  2877  30  100  36  82
etc.
</code></pre>
</li>
<li>
<p><code>--snv [file]</code> Somatic nucleotide variant read count data file.</p>

<p>Format: For each sample, there are two additional columns with (i) the number of reads of the somatic variant allele and (ii) the total read depth at that locus.</p>

<pre><code>1  1314  12  92   28  72  etc.
1  1287  47  99   32  80
1  2877  30  100  36  82
etc.
</code></pre>
</li>
<li><p><code>--pre [string:"./out"]</code>  Prefix for all output files.</p></li>
<li>
<p><code>--bias [file]</code>  The bias field for the read depth data. </p>

<p>This must be a filterHD <code>*posterior.*.txt</code> file, typically from a
 filterHD run on matched-normal read depth data, to estimate the
 technical read depth modulation.</p>
</li>
<li>
<p><code>--max-tcn [int]</code>  The maximum total copy number.</p>

<p>This is used as an upper limit for the total copy number genome wide (in all chr).
 If not specified, the normal copy number is used as limit for each chromosome.</p>

<p>This number should be chosen conservatively, since it increases the
 HMM dimensionality and can open up the possibility for spurious solutions. </p>
</li>
<li>
<p><code>--nmax [int:2]</code>  The maximum number of subclones to be tried.</p>

<p>All subclone numbers from 0 to <code>nmax</code> will be used and the one
 with maximum BIC chosen for output.</p>
</li>
<li><p><code>--force [int]</code>  Fix the number of subclones to be used.</p></li>
<li>
<p><code>--trials [int:1]</code>  The number of independent optimizations.</p>

<p>Global parameters are found numerically by local maximization of
 the total log-likelihood. The best result out of <code>trials</code> independent,
 randomly seeded,  runs will be used.</p>
</li>
<li>
<p><code>--mean-tcn [file]</code>  Use a fixed mean total copy number for SNV data. </p>

<p>For a SNV data analysis, the cloneHD output file
 ending <code>*mean_tcn.txt</code> from a CNA(+BAF) run can be supplied here. Since
 the subclonal decomposition can be different for SNVs, this option
 ensures that a reasonable mean total copy number is used.</p>
</li>
<li>
<p><code>--avail-cn [file]</code>  Use SNV copy number availablility constraint.</p>

<p>For a SNV data analysis, the cloneHD output file
 ending <code>*avail_cn.txt</code> from a CNA(+BAF) run can be supplied here. Since
 the subclonal decomposition can be different for SNVs, this option
 ensures that the SNV genotype is consistent with the fraction of
 cells in which this number of copies is available.
 Can only be used together with <code>--mean-tcn [file]</code>. In
 combination, this is a much stronger constraint than using
 <code>mean-tcn</code> alone.</p>
</li>
</ul><h3>
<a name="fuzzy-segmentation-options" class="anchor" href="#fuzzy-segmentation-options"><span class="octicon octicon-link"></span></a>Fuzzy segmentation options</h3>

<p>For data with persistence along the genome, a fuzzy segmentation can
be used based on the filterHD posterior jump probability (must be
<code>*jumps.txt</code> file). Data between potential jump sites, with a jump
probability of at least <code>min-jump</code>, is collapsed. The jump probability
is used in the HMM transition.</p>

<ul>
<li>   <code>--cna-jumps [file]</code>
</li>
<li>   <code>--baf-jumps [file]</code>
</li>
<li>   <code>--snv-jumps [file]</code>
</li>
<li>   <code>--min-jump [double:0.01]</code> </li>
</ul><h2>
<a name="parameter-options-1" class="anchor" href="#parameter-options-1"><span class="octicon octicon-link"></span></a>Parameter options</h2>

<p>The shape parameter for the over-dispersed emission models
(Negative-Binomial or Beta-Binomial). If not specified, the normal
models are used (Poisson or Binomial).</p>

<ul>
<li>   <code>--cna-shape [double:inf]</code>
</li>
<li>   <code>--baf-shape [double:inf]</code>
</li>
<li>   <code>--snv-shape [double:inf]</code>
</li>
</ul><p>The rate for indiviual random emissions per data set. Can be learned
with filterHD for data with persistence.</p>

<ul>
<li>   <code>--cna-rnd [double:0.0]</code>
</li>
<li>   <code>--baf-rnd [double:0.0]</code>
</li>
<li>   <code>--snv-rnd [double:0.0]</code>
</li>
</ul><h2>
<a name="advanced-options-1" class="anchor" href="#advanced-options-1"><span class="octicon octicon-link"></span></a>Advanced options</h2>

<ul>
<li>
<p><code>--clones [file]</code>  Use fixed mass(es) and/or subclonal frequencies. </p>

<p>Either all mass parameters, or all subclonal frequencies, or both 
 can be given (for each sample in the data input). The likelihoods
 and posteriors will be computed under these conditions. 
 Remaining parameters will be learned.</p>

<p>Format: One line per sample. The first column, if greater than
 1.0, is interpreted as mass; the remaining as subclonal frequencies.</p>

<pre><code>30.0 0.64 0.12
28.0 0.31 0.23
</code></pre>

<p>More than one parameter set can be given (as a continued list). Then,
only the likelihoods are computed and printed to a file ending
<code>*llh-values.txt</code>.  Useful for mapping the log-likelihood surface
or comparing several given solutions.</p>
</li>
<li><p><code>--purity [file]</code>  Use fixed purities, i.e. lower bounds for the sum of
 subclonal frequencies. One line per sample.</p></li>
<li>
<p><code>--restarts [int:10]</code>  The number of perturbations in local random
 search mode.</p>

<p>This simple random search routine is used: after finding a local
 maximum of LLH, the best solution is perturbed and a new optimum
 is sought. </p>
</li>
<li><p><code>--seed [int]</code>  A fixed seed to make inferences reproducible.</p></li>
<li>
<p><code>--mass-gauging [0/1:1]</code>  Whether to use mass-gauging.</p>

<p>The optimization in the space of masses (seq depths per haploid
 DNA) and subclonal frequencies can suffer from many local
 optima.  To fix the mass(es), one can, for a given solution,
 assume that an occupied state is actually all-normal. All
 occupied states will be proposed to fix the mass(es) </p>
</li>
<li><p><code>--min-occ [double:0.01]</code>  The minimum occupancy of levels to be
 used for the mass gauging.</p></li>
<li><p><code>--print-all [0/1:0]</code>  If 1, the posterior for every observation
 is printed to files ending <code>*[cna/baf/snv].posterior.txt</code>. 
 If 0, only one line for each segment is printed.</p></li>
<li>
<p><code>--max-tcn [file]</code>  The maximum total copy number per chr and subclone.</p>

<p>This file should have the format: chr max1 max2 max3 etc., e.g.</p>

<pre><code>1  2
2  2
3  8  2
4  2
etc.
</code></pre>

<p>The first column is the chromosome, the next columns are the limits to be used for subclone 1, 2 etc.
 For subclones not specified, the limit in the last column is used. In the example above, subclone 1 has an upper limit of 8 total copies in chr3, for all other subclones and in all other chromosomes, the upper limit is 2. If only SNV data is provided (and <code>--avail-cn [file]</code> is not given), this is used to fix the total number of copies. If <code>--max-tcn</code> is not given, cloneHD uses the normal copy number for each chr.</p>
</li>
<li><p><code>--learn-priors [0/1:0]</code> For snv-mode only: if 1, then the parameters
 for the multiplicative genotype priors are learned.</p></li>
<li>
<p><code>--chr [file]</code>  Set normal copy numbers.</p>

<p>The normal copy number for every single
 chromosome can be specified. This is needed only for non-human DNA. If not
 given, human DNA is assumed and the sex is inferred from the
 presence or absence of chr 24 (= chr Y) in the input data.</p>
</li>
<li><p><code>--snv-fprate [double:1.0e-4]</code>  The false positive rate for SNVs,
 i.e. rate of SNV data points of genotype all-0.</p></li>
<li><p><code>--snv-fpfreq [double:0.01]</code>  The typical frequency of false positive SNVs.</p></li>
<li><p><code>--snv-pen [double:0.01]</code>  The penalty for higher than expected
 genotypes.</p></li>
<li><p><code>--baf-pen [double:1.0]</code>  The penalty for complex minor allele
 status.</p></li>
<li><p><code>--cna-jump [double:-1.0]</code></p></li>
<li>   <code>--baf-jump [double:-1.0]</code>
</li>
<li>   <code>--snv-jump [double:-1.0]</code>
</li>
</ul><p>A constant jump probability per base pair. If -1, then observations are
uncorrellated along the genome. Can be learned with filterHD. No fuzzy
data segmentation is performed.  Useful in combination with
<code>--clones</code>, where very high-definition information
available. Using this option will change the posterior output file format.</p>

<h2>
<a name="bulk-options" class="anchor" href="#bulk-options"><span class="octicon octicon-link"></span></a>Bulk options</h2>

<p>These options are only needed if the sequenced cell population is a mixture of a
diverse bulk, with known allele frequency profile, and a number of
subclones with unknown genotypes and frequencies. Allele frequency
data is input with <code>--snv</code>. Data segmentation can be used with
<code>--snv-jumps</code>.  Read depth data can also be specified with <code>--cna</code>. </p>

<ul>
<li>
<p><code>--bulk-mean [double]</code>  The bulk allele frequency profile. </p>

<p>Must be a filterHD <code>*posterior.*.txt</code> file. Only the posterior mean is used.</p>
</li>
<li>
<p><code>--bulk-prior [file]</code>  The bulk allele frequency profile. </p>

<p>Must be a filterHD <code>*posterior.*.txt</code> file. The whole posterior
 distribution is used (run filterHD with <code>--dist 1</code> to obtain it).</p>
</li>
<li><p><code>--bulk-updates [int:0]</code>  The number of Bayesian updates of the
 bulk allele frequency profile (if <code>--bulk-prior</code> was used).</p></li>
<li><p><code>--bulk-fix [double:0.0]</code>  Use a flat and fixed bulk allele
 frequency profile.</p></li>
</ul><h2>
<a name="technical-options" class="anchor" href="#technical-options"><span class="octicon octicon-link"></span></a>Technical options</h2>

<p>The grid sizes for the pre-computed emission probabilities if fuzzy data segmentation is used.</p>

<ul>
<li>   <code>--cna-grid [int:300]</code><br>
</li>
<li>   <code>--baf-grid [int:100]</code> </li>
<li>   <code>--snv-grid [int:100]</code><br>
</li>
</ul><h1>
<a name="tips-and-tricks" class="anchor" href="#tips-and-tricks"><span class="octicon octicon-link"></span></a>Tips and tricks</h1>

<ul>
<li><p>Pre-filtering of data can be very important. If filterHD predicts
many more jumps than you would expect, it might be necessary to
filter the data, removing very short segments (with
<code>--filter-shortSeg 10</code>).</p></li>
<li><p>Make sure that the bias field for the tumor CNA data is
meaningful. If a matched normal sample was sequenced with the same
pipeline, its read depth profile, as predicted by filterHD, can be used as a
bias field for the tumor CNA data. Follow the logic of the example
given here.</p></li>
<li><p>filterHD can sometimes run into local optima. It might be useful to
fix initial values for the parameters via <code>--jumpi [double]</code> etc.</p></li>
<li><p>By default, cloneHD runs with mass gauging enabled. This seems like
an overkill, but is actually quite useful because you can see some
alternative explanations during the course of the analysis.</p></li>
<li><p>Don't put too much weight on the BIC criterion. It was calibrated
using simulated data. For real data, it should be supplied with
common sense and biological knowledge. Use <code>--force [int]</code> to use a
fixed number of subclones.</p></li>
<li><p>For exome sequencing data, the read depth bias can be enormous. Use rather, if
available, the jumps seen in the BAF data for both CNA and BAF.</p></li>
</ul>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">cloneHD maintained by <a href="https://github.com/andrej-fischer">andrej-fischer</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
